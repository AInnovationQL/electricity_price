{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "737fcb6f-ebe1-4021-a1e0-c5719536bca3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Machine Learning Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d073da96-543c-46d1-bf71-192cd59db567",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "This notebook includes:\n",
    "- Call the jupyter notebook \"notebook/feature_engineering.ipynb\" to carry out the feature enginnering with flexible parameterization.\n",
    "- Comparision of five ML models: TabPFN regression, Random Forest regression, XGBoost regression, LightGBM regression, CatBoost regression.\n",
    "- Select the best-performing model and use it to predict day ahead electricity price changes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662a7cdd-3678-4d93-9ff2-a84175d8fa16",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# 1. Configuration and Parameterization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "232ee655-36b1-475a-99b4-3f70efaa0d1b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#configuration\n",
    "# path \n",
    "OUTPUT_DIR = \"data/\"\n",
    "SUBMISSION_PATH = f\"{OUTPUT_DIR}y_test_submission.csv\"\n",
    "\n",
    "RANDOM_STATE=47 # random seed\n",
    "TEST_SIZE=0.1 # portion of the test set\n",
    "n_splits=5 # step for cross validation\n",
    "\n",
    "# parameter space\n",
    "param_grid_call = {\n",
    "    'input_dir': [\"data/\"], # input dir as parameter, which allows this notebook to access and call other notebooks flexibly.\n",
    "    'missing_outlier_r_threshold': [6, 7], # threshold for the removal of rows with lage number of missing and outliers, recommend 6 or 7\n",
    "    'missing_outlier_c_threshold': [200], # threshold for the removal of columns with lage number of missing and outliers, recommend 200\n",
    "    'impute_method': ['mean', 'mice', 'knn', 'interp'], # imputation methods: 'zero', 'mean', 'median', 'mice', 'knn', 'interp'\n",
    "    'corr_method': ['spearman'], # 'pearson' or 'spearman'\n",
    "    'corr_threshold': [0.85, 0.9], # correlation threshold to determine close related feature pairs, recommend 0.8, 0.85 or 0.9\n",
    "    'target_corr_threshold': [0.01], # correlation threshold to determine close related feature pairs, recommend 0.01 or 0.02\n",
    "    'null_importance_threshold': [90], # threshold for null importance, recommend 80 or 90\n",
    "    'OnlyOrigNumFeatures': [True, False], # whether only provide the original numerical features for the learning\n",
    "    'KeepOrigNumFeatures': [True, False], # whether keep original numerical features for the learning\n",
    "    'UseTimeSeries': [True, False], # use the complete time series of trian and test data set\n",
    "    'RemoveMissingOutlier': [True, False] # whether remove the rows and columns with a large number of missing and outlier values\n",
    "}\n",
    "\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [5, 10, 15],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "param_grid_xgb = {\n",
    "    'max_depth': [3, 6, 9],  \n",
    "    'reg_alpha': [0.1, 0.2, 0.3], \n",
    "    'n_estimators': [50],\n",
    "    'learning_rate': [0.05, 0.1, 0.2],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'subsample': [0.6, 0.8, 1.0]\n",
    "}\n",
    "\n",
    "param_grid_lgb = {\n",
    "    'boosting_type': ['gbdt'],\n",
    "    'num_leaves': [14, 15, 30, 50],\n",
    "    'learning_rate': [0.045, 0.05, 0.1, 0.2],\n",
    "    'feature_fraction': [0.6, 0.8, 0.9],\n",
    "    'bagging_fraction': [0.6, 0.8, 0.9],\n",
    "    'bagging_freq': [3, 5, 7]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58d7a577-6aae-491b-ae49-73d2832b0427",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import of libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import papermill as pm\n",
    "import scrapbook as sb\n",
    "import torch\n",
    "import shutil\n",
    "\n",
    "from pathlib import Path\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from tabpfn import TabPFNRegressor\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "if not torch.cuda.is_available():\n",
    "    raise SystemError('GPU device not found. For fast training, please enable GPU. See section above for instructions.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525712ff-408e-4087-926e-4aa96863198b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# 2. Call the notebook for feature engineering and choose the best engineered dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ada4754-ccbc-406d-9cfb-1153f2fe0bc2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# clear the experiment folder \n",
    "folder = Path(\"experiment\")\n",
    "shutil.rmtree(folder, ignore_errors=True)\n",
    "folder.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0b714c-227a-4d16-8464-2c040c942757",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab3dfaef7cad4b44ac7f4a825e9bec50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Executing:   0%|          | 0/108 [00:00<?, ?cell/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "362e1f429e624b2ea231d046e29161b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Executing:   0%|          | 0/108 [00:00<?, ?cell/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# call the jupyter notebook \"notebook/feature_engineering.ipynb\" with parameterization\n",
    "best_score = (-1, 0)\n",
    "best_params = {}\n",
    "_X_train = pd.DataFrame()\n",
    "_y_train = pd.DataFrame()                \n",
    "_X_test = pd.DataFrame() \n",
    "\n",
    "for params in ParameterGrid(param_grid_call):\n",
    "    try:\n",
    "        output = pm.execute_notebook(\n",
    "            input_path=\"notebook/feature_engineering.ipynb\",\n",
    "            output_path=f'experiment/experiment_{hash(str(params))}.ipynb',\n",
    "            parameters=params\n",
    "        )\n",
    "\n",
    "        nb = sb.read_notebook(f'experiment/experiment_{hash(str(params))}.ipynb')\n",
    "        spearman = nb.scraps[\"spearman\"].data\n",
    "        r2 = nb.scraps[\"r2\"].data\n",
    "        score = (spearman, r2)\n",
    "        _X_train = nb.scraps[\"X_train\"].data\n",
    "        _y_train = nb.scraps[\"y_train\"].data                 \n",
    "        _X_test = nb.scraps[\"X_test\"].data \n",
    "    except:\n",
    "        score = (-1, 0)\n",
    "        print(f'experiment_{hash(str(params))}.ipynb stopped')\n",
    "        display(params)\n",
    "        display(best_score)\n",
    "        display(best_params)\n",
    "    \n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_params = params\n",
    "        best_X_train = _X_train\n",
    "        best_y_train = _y_train\n",
    "        best_X_test = _X_test\n",
    "\n",
    "print(\"The optimal select feature engineering parameter:\")\n",
    "display(best_params)\n",
    "print(f\"The best prediction score (spearman corr, r2):\\n {best_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f886d29b-6292-4222-9d4a-778f89b819d8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# 3. Comparision of ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6625905-6ad0-4b19-ba90-02ee279054c2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get laerning data\n",
    "X_train = best_X_train.drop('ID', axis=1).copy()\n",
    "X_test = best_X_test.drop('ID', axis=1).copy()\n",
    "y_train = best_y_train['TARGET'].copy()\n",
    "submission = best_X_test[['ID']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703b6700-1e8c-4ce9-8c42-0c50e1e42b19",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# compare ML regression models\n",
    "models = [\n",
    "    ('TabPFN', TabPFNRegressor(random_state=42, device=\"cuda\")),\n",
    "    ('RandomForest', RandomForestRegressor(random_state=42)),\n",
    "    ('XGBoost', XGBRegressor(random_state=42)),\n",
    "    ('LightGBM', LGBMRegressor(random_state=42, verbose=0)),\n",
    "    ('CatBoost', CatBoostRegressor(random_state=42, verbose=0))\n",
    "]\n",
    "\n",
    "X_train_, X_test_, y_train_, y_test_ = train_test_split(X_train, y_train, test_size=0.15, random_state=42)\n",
    "corr_spearman = {}\n",
    "score_mse = {}\n",
    "\n",
    "for name, model in models:\n",
    "    print(name)\n",
    "    model.fit(X_train_.to_numpy(), y_train_.to_numpy())\n",
    "    y_pred_ = model.predict(X_test_)\n",
    "    corr = spearmanr(y_test_, y_pred_).correlation\n",
    "    corr_spearman[name] = corr * 100\n",
    "    print(f\"spearman = {corr_spearman[name]:.4f}%\")\n",
    "    rmse = root_mean_squared_error(y_test_, y_pred_)\n",
    "    score_mse[name] = rmse\n",
    "    print(f\"rmse = { rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff948c5-ba27-4393-84b8-7c9ab48c7154",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot model comparision results\n",
    "df = pd.DataFrame(list(corr_spearman.items()), columns=['Model', 'Spearman(%)'])\n",
    "df = df.merge(pd.DataFrame(list(score_mse.items()), columns=['Model', 'RMSE']), on='Model', how='left')\n",
    "ax = df.plot(x='Model', y=['Spearman(%)','RMSE'], kind='bar', figsize=(10, 6))\n",
    "ax.set_title('Model Comparison')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd722ef-55e0-40f5-b483-56b447d5abde",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get the best prediction model\n",
    "df_compare = df.copy()\n",
    "df_compare.sort_values(by=['Spearman(%)', 'RMSE'], ascending=[False, True], ignore_index=True, inplace=True)\n",
    "best_model = str(df_compare['Model'].values[0])\n",
    "display(best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5838458-7d77-41f2-b43d-ec215d0ae73a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# 4. Prediction using the best performed ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db9ea12-fa1a-44c7-a728-5543ad39a59a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define the TabPFN regression model with cross validation\n",
    "def TabPFNRegressorCV(X, y, Xt):\n",
    "    test_predictions = []\n",
    "    oof_predictions = np.zeros(len(X))\n",
    "\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_STATE)\n",
    "    \n",
    "    print(f\"Starting {n_splits}-fold cross-validation...\")\n",
    "    \n",
    "    for fold, (train_index, val_index) in enumerate(kf.split(X, y)):\n",
    "        print(f\"\\n--- Fold {fold+1}/{n_splits} ---\")\n",
    "        \n",
    "        Xtrain, Xval = X.iloc[train_index], X.iloc[val_index]\n",
    "        ytrain, yval = y.iloc[train_index], y.iloc[val_index]\n",
    "    \n",
    "        model = TabPFNRegressor(random_state=RANDOM_STATE, device=\"cuda\")\n",
    "        model.fit(Xtrain, ytrain)\n",
    "        \n",
    "        val_preds = model.predict(Xval)\n",
    "        oof_predictions[val_index] = val_preds\n",
    "        r2 = root_mean_squared_error(yval, val_preds)\n",
    "        print(f\"rmse score on validation set for Fold {fold+1}: {r2:.5f}\")\n",
    "        \n",
    "        test_preds = model.predict(Xt)\n",
    "        test_predictions.append(test_preds)\n",
    "    \n",
    "    return np.mean(test_predictions, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ccfdc8-b009-4988-8d97-0d0f64229525",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define the Random Forest regression model with cross validation\n",
    "def RandomForestRegressorCV(X, Y):\n",
    "    X_train_, X_test_, y_train_, y_test_ = train_test_split(X, Y, test_size=TEST_SIZE, random_state=RANDOM_STATE)\n",
    "\n",
    "    reg = RandomForestRegressor(random_state=RANDOM_STATE)\n",
    "    grid_search = GridSearchCV(estimator=reg, param_grid=param_grid_rf, cv=n_splits, scoring='neg_root_mean_squared_error', verbose=2, n_jobs=-1)\n",
    "    grid_search.fit(X_train_, y_train_)\n",
    "    print(\"Best parameters: \", grid_search.best_params_)\n",
    "\n",
    "    y_pred = grid_search.predict(X_test_)\n",
    "\n",
    "    rmse = root_mean_squared_error(y_test_, y_pred)\n",
    "    print('rmse on test set: {:.2f}'.format(rmse))\n",
    "\n",
    "    return {\"parameters\": grid_search.best_params_, \"model\": grid_search}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8041a0e2-612a-458e-8388-3b9cf396370d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define the XGBoost regression model with cross validation\n",
    "def XGBoostRegressorCV(X, Y):\n",
    "    X_train_, X_test_, y_train_, y_test_ = train_test_split(X, Y, test_size=TEST_SIZE, random_state=RANDOM_STATE)\n",
    "\n",
    "    reg = XGBRegressor(booster='gbtree', objective='reg:squarederror')\n",
    "    grid_search = GridSearchCV(estimator=reg, param_grid=param_grid_xgb, cv=n_splits, scoring='neg_mean_squared_error', return_train_score=True, verbose=1, n_jobs=-1)\n",
    "    grid_search.fit(X_train_, y_train_)\n",
    "\n",
    "    best_parameters = grid_search.best_params_\n",
    "    best_rmse = -grid_search.best_score_\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    y_pred = best_model.predict(X_test_)\n",
    "    rmse = root_mean_squared_error(y_test_, y_pred)\n",
    "\n",
    "    print(\"Best parameters: \", grid_search.best_params_)\n",
    "    print(\"Best score: \", best_rmse)\n",
    "    print(\"MSE on test data: \", rmse)\n",
    "\n",
    "    return {\"parameters\": best_parameters, \"model\":best_model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e3ad42-36dd-48a6-848e-58c5bfc28a73",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define the lightGBM regression model with cross validation\n",
    "def LightGBMRegressorCV(X, Y):\n",
    "    X_train_, X_test_, y_train_, y_test_ = train_test_split(X, Y, test_size=TEST_SIZE, random_state=RANDOM_STATE)\n",
    "    \n",
    "    model = LGBMRegressor(objective='regression', metric='mse')\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid_lgb, cv=n_splits, scoring='neg_root_mean_squared_error', verbose=0, n_jobs=-1)\n",
    "    grid_search.fit(X_train_, y_train_)\n",
    "    \n",
    "    best_parameters = grid_search.best_params_\n",
    "    best_rmse = -grid_search.best_score_\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    y_pred = best_model.predict(X_test_)\n",
    "    rmse = root_mean_squared_error(y_test_, y_pred)\n",
    "\n",
    "    print(\"Best parameters: \", best_parameters)\n",
    "    print(\"Best score: \", best_rmse)\n",
    "    print(\"rmse on test data: \", rmse)\n",
    "\n",
    "    return {\"parameters\": best_parameters, \"model\":best_model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b4d8b7-5881-483c-9968-1d66638c396c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def CatBoostRegressorCV(X, y, Xt):\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_STATE)\n",
    "    oof = np.zeros(len(X))\n",
    "    test_predictions = []\n",
    "\n",
    "    for fold, (tr, va) in enumerate(kf.split(X), 1):\n",
    "        X_tr, X_va = X.iloc[tr], X.iloc[va]\n",
    "        y_tr, y_va = y.iloc[tr], y.iloc[va]\n",
    "\n",
    "        train_pool = Pool(X_tr, y_tr)\n",
    "        valid_pool = Pool(X_va, y_va)\n",
    "\n",
    "        model = CatBoostRegressor(n_estimators=100, max_depth=5, learning_rate=0.1, random_seed=RANDOM_STATE, silent=True)\n",
    "        model.fit(train_pool, eval_set=valid_pool, verbose=False)\n",
    "\n",
    "        pred_va = model.predict(valid_pool)\n",
    "        oof[va] = pred_va\n",
    "        \n",
    "        test_preds = model.predict(Xt)\n",
    "        test_predictions.append(test_preds)\n",
    "\n",
    "    score = root_mean_squared_error(y, oof)\n",
    "    print(f'rmse on data set: {score:.2f}')\n",
    "    \n",
    "    return np.mean(test_predictions, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a5809a-852d-41f1-941d-090b607b6f60",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# forecast day ahead price changes with the best performed ML model\n",
    "if best_model == 'TabPFN':\n",
    "    print('Prediction using the TabPFN regression:')\n",
    "    submission['TARGET'] = TabPFNRegressorCV(X_train, y_train, X_test)\n",
    "elif best_model == 'RandomForest':\n",
    "    print('Prediction using the Random Forest regression:')\n",
    "    ml_model = RandomForestRegressorCV(X_train, y_train)\n",
    "    submission['TARGET'] = ml_model['model'].predict(X_test)\n",
    "elif best_model == 'XGBoost':\n",
    "    print('Prediction using the XGBoost regression:')\n",
    "    ml_model = XGBoostRegressorCV(X_train, y_train)\n",
    "    submission['TARGET'] = ml_model['model'].predict(X_test)\n",
    "elif best_model == 'CatBoost':\n",
    "    print('Prediction using the CatBoost regression:')\n",
    "    submission['TARGET'] = CatBoostRegressorCV(X_train, y_train, X_test)\n",
    "else:\n",
    "    print('Prediction using the LightGBM regression:')\n",
    "    ml_model = LightGBMRegressorCV(X_train, y_train)\n",
    "    submission['TARGET'] = ml_model['model'].predict(X_test)\n",
    "\n",
    "submission.to_csv(SUBMISSION_PATH, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qrt310",
   "language": "python",
   "name": "qrt310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
